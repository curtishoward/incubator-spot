From c057ad5522db10e9e23760e0b0b1ce6ded8d8086 Mon Sep 17 00:00:00 2001
From: curtis <curtis@cloudera.com>
Date: Sat, 21 Apr 2018 09:28:39 -0400
Subject: [PATCH] [ENV-252] Add Hive output option to align step schema with
 target table (#174)

---
 docs/configurations.adoc                           |  3 ++
 .../cloudera/labs/envelope/output/HiveOutput.java  | 37 ++++++++++++++++++-
 .../labs/envelope/output/TestHiveOutput.java       | 41 +++++++++++++++++++++-
 3 files changed, 79 insertions(+), 2 deletions(-)

diff --git a/docs/configurations.adoc b/docs/configurations.adoc
index 4a15fec..50d2887 100644
--- a/docs/configurations.adoc
+++ b/docs/configurations.adoc
@@ -916,6 +916,9 @@ Output configurations belong to data steps, and have the `steps.[stepname].outpu
 |partition.by
 |Optional. The list of Hive table partition names to dynamically partition the write by.
 
+|align.columns
+|If `true` then Envelope will attempt to align the output schema by matching (case-insensitive, unless `spark.sql.caseSensitive` is set) the step's column names with those of the target Hive table.  Step columns without a match in the target table will not be included in the aligned output, and similarly, target Hive table columns not available in the step schema will be NULL.
+
 |options
 |Used to pass additional configuration parameters. The parameters are set as a Map object and passed directly to the Spark DataFrameWriter.
 
diff --git a/lib/src/main/java/com/cloudera/labs/envelope/output/HiveOutput.java b/lib/src/main/java/com/cloudera/labs/envelope/output/HiveOutput.java
index 371c671..e4c79b6 100644
--- a/lib/src/main/java/com/cloudera/labs/envelope/output/HiveOutput.java
+++ b/lib/src/main/java/com/cloudera/labs/envelope/output/HiveOutput.java
@@ -20,11 +20,16 @@ package com.cloudera.labs.envelope.output;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
+import java.util.Arrays;
+import java.util.ArrayList;
+import java.util.HashSet;
 
 import org.apache.spark.sql.DataFrameWriter;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
 import org.apache.spark.sql.SaveMode;
+import org.apache.spark.sql.Column;
+import org.apache.spark.sql.functions;
 
 import com.cloudera.labs.envelope.load.ProvidesAlias;
 import com.cloudera.labs.envelope.plan.MutationType;
@@ -32,6 +37,7 @@ import com.cloudera.labs.envelope.utils.ConfigUtils;
 import com.google.common.collect.Sets;
 import com.typesafe.config.Config;
 import com.typesafe.config.ConfigValue;
+import com.cloudera.labs.envelope.spark.Contexts;
 
 import scala.Tuple2;
 
@@ -41,6 +47,8 @@ public class HiveOutput implements BulkOutput, ProvidesAlias {
   public final static String PARTITION_BY_CONFIG = "partition.by";
   public final static String LOCATION_CONFIG = "location";
   public final static String OPTIONS_CONFIG = "options";
+  public final static String ALIGN_COLUMNS_CONFIG = "align.columns";
+  public final static String SPARK_SQL_CASE_SENSITIVE_CONFIG = "spark.sql.caseSensitive";
 
   private Config config;
   private ConfigUtils.OptionMap options;
@@ -77,7 +85,7 @@ public class HiveOutput implements BulkOutput, ProvidesAlias {
   public void applyBulkMutations(List<Tuple2<MutationType, Dataset<Row>>> planned) {    
     for (Tuple2<MutationType, Dataset<Row>> plan : planned) {
       MutationType mutationType = plan._1();
-      Dataset<Row> mutation = plan._2();
+      Dataset<Row> mutation = (getAlignColumns()) ? alignColumns(plan._2()) : plan._2();
       DataFrameWriter<Row> writer = mutation.write();
 
       if (hasPartitionColumns()) {
@@ -108,6 +116,29 @@ public class HiveOutput implements BulkOutput, ProvidesAlias {
     return Sets.newHashSet(MutationType.INSERT, MutationType.OVERWRITE);
   }
 
+  public Dataset<Row> alignColumns(Dataset<Row> input) {
+    Boolean caseSensitive = Contexts.getSparkSession().sparkContext().getConf().
+                            getBoolean(SPARK_SQL_CASE_SENSITIVE_CONFIG, false);
+
+    Set<String> inputCols = new HashSet<String>();
+    for (String col : Arrays.asList(input.schema().fieldNames())) {
+      inputCols.add((caseSensitive) ? col : col.toLowerCase());
+    }
+
+    List<String> tableCols = new ArrayList<String>();
+    for (String col : Contexts.getSparkSession().table(getTableName()).schema().fieldNames()) {
+      tableCols.add((caseSensitive) ? col : col.toLowerCase());
+    }
+
+    List<Column> alignedCols = new ArrayList<Column>();
+    for (String column : tableCols) {
+      alignedCols.add((inputCols.contains(column)) ? functions.col(column) : 
+                                                     functions.lit(null).alias(column));
+    } 
+
+    return input.select(alignedCols.toArray(new Column[alignedCols.size()]));
+  }
+
   private boolean hasPartitionColumns() {
     return config.hasPath(PARTITION_BY_CONFIG);
   }
@@ -125,6 +156,10 @@ public class HiveOutput implements BulkOutput, ProvidesAlias {
     return config.getString(TABLE_CONFIG);
   }
 
+  private boolean getAlignColumns() {
+    return config.hasPath(ALIGN_COLUMNS_CONFIG) && config.getBoolean(ALIGN_COLUMNS_CONFIG);
+  }
+
   @Override
   public String getAlias() {
     return "hive";
diff --git a/lib/src/test/java/com/cloudera/labs/envelope/output/TestHiveOutput.java b/lib/src/test/java/com/cloudera/labs/envelope/output/TestHiveOutput.java
index e6c072b..ec2b87c 100644
--- a/lib/src/test/java/com/cloudera/labs/envelope/output/TestHiveOutput.java
+++ b/lib/src/test/java/com/cloudera/labs/envelope/output/TestHiveOutput.java
@@ -19,17 +19,22 @@ package com.cloudera.labs.envelope.output;
 
 import com.cloudera.labs.envelope.plan.MutationType;
 import com.cloudera.labs.envelope.spark.Contexts;
+import com.cloudera.labs.envelope.utils.RowUtils;
 import com.typesafe.config.Config;
 import com.typesafe.config.ConfigFactory;
 import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.Set;
 import mockit.Deencapsulation;
+import com.google.common.collect.Lists;
 import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.spark.sql.SparkSession;
 import org.apache.spark.sql.Dataset;
 import org.apache.spark.sql.Row;
+import org.apache.spark.sql.types.StructType;
 import org.junit.After;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotNull;
@@ -124,6 +129,40 @@ public class TestHiveOutput {
     assertEquals("Invalid option", "badabing", optionsMap.get("hive-option"));
   }
 
+  @Test
+  public void alignColumns() throws Exception {
+    SparkSession spark = Contexts.getSparkSession();
+    String targetTable = "temp_target_table";
+    Map<String, Object> paramMap = new HashMap<>();
+    paramMap.put(HiveOutput.TABLE_CONFIG, targetTable);
+    config = ConfigFactory.parseMap(paramMap);
+    HiveOutput hiveOutput = new HiveOutput();
+    hiveOutput.configure(config);
+
+    StructType targetSchema = RowUtils.structTypeFor(
+        Lists.newArrayList("zip_code", "city", "state", "Fname", "lname"), 
+        Lists.newArrayList("int", "string", "string", "string", "string"));
+    spark.createDataFrame(spark.emptyDataFrame().rdd(), targetSchema)
+         .createOrReplaceTempView(targetTable);
+
+    String stepDef = "SELECT \"foo\" AS fname, \"bar\" AS lname, 18 AS age, 12345 AS ZIP_CODE";
+    Dataset<Row> stepDataframe = spark.sql(stepDef);
+
+    Dataset<Row> alignedDataframe =  hiveOutput.alignColumns(stepDataframe);
+
+    Row alignedData = alignedDataframe.first();
+    String expectedDef = "SELECT 12345 AS zip_code, NULL as city, NULL as state, " +
+                         "\"foo\" AS fname, \"bar\" AS lname";
+    Row expectedData = spark.sql(expectedDef).first();
+
+    assertEquals(expectedData.size(), alignedData.size());
+    for (int i = 0; i < alignedData.length(); i++) {
+      assertEquals(alignedData.get(i), expectedData.get(i));
+    }
+    assertEquals(Arrays.asList(alignedData.schema().fieldNames()), 
+                 Arrays.asList(expectedData.schema().fieldNames()));
+  }
+
   @Ignore ("Needs review of Hive temporary folder creation")
   @Test
   public void applyBulkMutations() throws Exception {
@@ -147,4 +186,4 @@ public class TestHiveOutput {
     assertTrue("OVERWRITE not supported", mutationTypes.contains(MutationType.OVERWRITE));
   }
 
-}
\ No newline at end of file
+}
-- 
1.8.3.1

